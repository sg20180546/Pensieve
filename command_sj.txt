================================================================================
                    Pensieve Server Setup & Execution Guide
          Stateful LLM Serving with KV Cache Management (EuroSys 2025)
================================================================================

## PART 1: Environment Setup & Prerequisites
================================================================================

# 1.1 Check system information
echo "=== System Information ==="
uname -a
nvidia-smi  # Check GPU availability and CUDA version
python --version
pip --version

# 1.2 Verify CUDA is available (required for GPU inference)
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'CUDA version: {torch.version.cuda}'); print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\"}')"

# 1.3 Check free GPU memory
nvidia-smi --query-gpu=memory.free --format=csv,nounits


## PART 2: Install Dependencies
================================================================================

# 2.1 Navigate to project directory
cd /Users/sj/pensieve

# 2.2 Create Python virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 2.3 Upgrade pip, setuptools, wheel
pip install --upgrade pip setuptools wheel

# 2.4 Install core dependencies
pip install -r requirements.txt

# 2.5 Verify installations
python -c "import torch, transformers, numpy; print('All imports successful!')"

# 2.6 Install Pensieve in development mode
pip install -e .


## PART 3: Download Models (First Time Only)
================================================================================

# 3.1 Pre-download models to avoid network issues during inference
# GPT-2 (smallest, fastest for testing)
python -c "from transformers import GPT2Tokenizer, GPT2LMHeadModel; GPT2Tokenizer.from_pretrained('gpt2'); GPT2LMHeadModel.from_pretrained('gpt2'); print('GPT-2 downloaded')"

# 3.2 Meta-Llama-3-8B (Main model for testing - REQUIRES HF LOGIN)
# IMPORTANT: Must login to HuggingFace first
# huggingface-cli login  ← Do this BEFORE downloading
python -c "
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
print('Downloading Meta-Llama-3-8B tokenizer...')
AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B')
print('✓ Tokenizer downloaded')
print('Downloading Meta-Llama-3-8B model... (this may take 10-15 minutes, ~16GB)')
model = AutoModelForCausalLM.from_pretrained(
    'meta-llama/Meta-Llama-3-8B',
    torch_dtype=torch.bfloat16,
    device_map='auto'
)
print('✓ Meta-Llama-3-8B downloaded and cached')
"

# 3.3 Optional: Download other larger models
# OPT-125M (125M parameters)
# python -c "from transformers import AutoTokenizer, AutoModelForCausalLM; AutoTokenizer.from_pretrained('facebook/opt-125m'); AutoModelForCausalLM.from_pretrained('facebook/opt-125m'); print('OPT-125M downloaded')"

# OPT-350M (350M parameters, ~2GB)
# python -c "from transformers import AutoTokenizer, AutoModelForCausalLM; AutoTokenizer.from_pretrained('facebook/opt-350m'); AutoModelForCausalLM.from_pretrained('facebook/opt-350m'); print('OPT-350M downloaded')"

# OPT-13B (13B parameters, ~26GB - requires HuggingFace token)
# python -c "from transformers import AutoTokenizer, AutoModelForCausalLM; AutoTokenizer.from_pretrained('facebook/opt-13b'); AutoModelForCausalLM.from_pretrained('facebook/opt-13b', device_map='auto'); print('OPT-13B downloaded')"


## PART 4: Run Basic Tests
================================================================================

# 4.1 Run basic inference test (verify everything works)
python scripts/test_basic_inference.py

# Expected output: All 5 tests should pass


## PART 5: Run Pensieve Server - Main Demonstrations
================================================================================

# 5.1 BASIC DEMO: Simple multi-turn conversation (Pensieve mode)
echo "=== Running Pensieve Demo ==="
python main.py --mode pensieve --model gpt2 --max-new-tokens 32

# Expected output:
# - Demo conversations with session IDs
# - Statistics showing GPU/CPU cache usage
# - Timing information


# 5.2 BASELINE COMPARISON: Same demo with vLLM baseline (stateless)
echo "=== Running vLLM Baseline Demo ==="
python main.py --mode vllm --model gpt2 --max-new-tokens 32

# Expected output:
# - Same conversations but with stateless processing
# - Should be slower due to recomputing history


# 5.3 DIRECT COMPARISON: Run Pensieve vs vLLM side by side
echo "=== Comparing Pensieve vs vLLM ==="
python main.py --mode compare --model gpt2 --max-new-tokens 32

# Expected output:
# - Pensieve statistics
# - vLLM baseline statistics
# - Speedup ratio at the end
# - Target: >1.5x speedup on turn 5+


## PART 6: Interactive Mode - Multi-turn Conversation
================================================================================

# 6.1 Start interactive Pensieve session
echo "=== Starting Interactive Pensieve Session ==="
python main.py --mode pensieve --model gpt2 --interactive --max-new-tokens 20

# Usage:
# - Type: "Hello" <Enter>
# - Type: "Tell me about AI" <Enter>
# - Type: "What is machine learning?" <Enter>
# - Type: "stats" <Enter> to see cache statistics
# - Type: "exit" <Enter> to quit

# Expected behavior:
# - Turn 1: Fresh computation, slow
# - Turn 2+: Faster due to cached history (Pensieve advantage)


# 6.2 Compare with interactive vLLM baseline
echo "=== Starting Interactive vLLM Session ==="
python main.py --mode vllm --model gpt2 --interactive --max-new-tokens 20

# Same interaction pattern
# Expected: All turns similar speed (no cache reuse in stateless mode)


## PART 7: Advanced Testing with Different Models
================================================================================

# 7.1 Test with OPT-125M (larger model, better cache benefits)
echo "=== Testing with OPT-125M ==="
python main.py --mode pensieve --model facebook/opt-125m --gpu-cache 40 --max-new-tokens 32

# Expected: More visible speedup benefit

# 7.2 Test with OPT-350M (larger still)
# python main.py --mode pensieve --model facebook/opt-350m --gpu-cache 40 --max-new-tokens 32


## PART 8: Custom Cache Configuration
================================================================================

# 8.1 Pensieve with custom cache sizes
echo "=== Pensieve with Custom Cache Configuration ==="
python main.py \
  --mode pensieve \
  --model gpt2 \
  --gpu-cache 20 \
  --cpu-cache 50 \
  --max-new-tokens 32

# 8.2 Pensieve with limited GPU cache (test eviction)
echo "=== Testing Eviction with Limited GPU Cache ==="
python main.py \
  --mode pensieve \
  --model gpt2 \
  --gpu-cache 2 \
  --cpu-cache 10 \
  --max-new-tokens 32

# Expected: Observe CPU cache utilization and eviction behavior


## PART 9: Performance Profiling
================================================================================

# 9.1 Profile single turn
echo "=== Single Turn Timing ==="
python main.py --mode pensieve --model gpt2 --max-new-tokens 10

# 9.2 Profile multiple sessions
echo "=== Multiple Session Timing ==="
python main.py --mode compare --model gpt2 --max-new-tokens 20

# 9.3 Measure speedup at different turns (manual testing)
# - Record time for turn 1 (no cache)
# - Record time for turn 5 (with cache)
# - Calculate speedup = time_turn1 / time_turn5


## PART 10: Debugging & Analysis
================================================================================

# 10.1 Check GPU memory usage while running
# In another terminal, run:
watch -n 1 nvidia-smi

# 10.2 Run with verbose output
python -u main.py --mode pensieve --model gpt2 2>&1 | tee pensieve_output.log

# 10.3 Run with profiling (Python profiler)
python -m cProfile -s cumulative -o pensieve.prof main.py --mode pensieve --model gpt2
python -c "import pstats; p = pstats.Stats('pensieve.prof'); p.sort_stats('cumulative').print_stats(30)"


## PART 11: Batch Testing (Multiple Models & Configurations)
================================================================================

# 11.1 Create test results directory
mkdir -p results

# 11.2 Test Pensieve on different models
echo "Testing Pensieve mode on multiple models..."
for model in gpt2 facebook/opt-125m; do
    echo "Testing $model..."
    python main.py --mode pensieve --model $model --max-new-tokens 20 > results/pensieve_${model//\//_}.txt 2>&1
done

# 11.3 Test vLLM baseline on different models
echo "Testing vLLM mode on multiple models..."
for model in gpt2 facebook/opt-125m; do
    echo "Testing $model..."
    python main.py --mode vllm --model $model --max-new-tokens 20 > results/vllm_${model//\//_}.txt 2>&1
done

# 11.4 Compare results
cat results/*.txt


## PART 12: Test Chunk Pinning Mechanism (Critical for Concurrent Safety)
================================================================================

# This tests the pinning mechanism that prevents chunks from being evicted
# while a batch is executing. This is critical for concurrent multi-session execution.
#
# Without pinning: Session 1 loses its chunks if Session 2 makes a request
# With pinning: Session 1 chunks are protected during batch execution

# 12.1 Run chunk pinning tests
python scripts/test_chunk_pinning.py

# Expected output:
# ✅ TEST 1: Basic Pin/Unpin Operations
#   ✓ Chunk stored
#   ✓ Chunk not pinned initially
#   ✓ Session pinned
#   ✓ Session unpinned
#
# ✅ TEST 2: Pinned Chunks Protected From Eviction
#   ✓ Chunks stored
#   ✓ Session pinned
#   ✓ Session chunks still in cache after eviction attempt
#
# ✅ TEST 3: Multiple Pinned Sessions
#   ✓ Multiple sessions pinned/unpinned simultaneously
#
# ✅ TEST 4: Pinning During Batch Execution (Simulated)
#   ✓ Batch chunks protected while pinned
#   ✓ Batch chunks can be evicted after unpinned
#
# ✅ TEST 5: Scheduler Request Deferral (NEW - Critical!)
#   ✓ Scheduler defers pinned sessions gracefully
#   ✓ Only unpinned sessions added to batch
#   ✓ Deferred requests return to queue
#   ✓ No "all chunks pinned" stalls

# 12.2 What this tests:
# - Scenario: Session A,B,C executing (pinned), Session D arrives
# - Without fix: D added to batch → eviction tries A,B,C → all pinned → failure!
# - With fix: D deferred → only evict unpinned sessions → success!


## PART 13: Expected Performance Benchmarks
================================================================================

# Expected results (from paper, OPT-13B):
# - Single GPU throughput: 1.36x vs vLLM
# - Turn 5 prefill speedup: 2.0x
# - Turn 10 prefill speedup: 3.5x
# - GPU cache hit rate: 70%
# - CPU cache hit rate: 20%

# Note: With GPT-2, speedup may be less visible due to:
# - Small model size (less KV cache memory)
# - Simple model architecture
# Better results expected with OPT-13B or larger models


## PART 14: Troubleshooting
================================================================================

# 14.1 If CUDA not available
# Set CPU-only mode:
export CUDA_VISIBLE_DEVICES=""
python main.py --mode pensieve --device cpu --model gpt2

# 14.2 If out of memory
# Reduce cache sizes:
python main.py --mode pensieve --gpu-cache 5 --cpu-cache 20 --model gpt2

# 14.3 If model download fails
# Clear cache and retry:
rm -rf ~/.cache/huggingface/
python -c "from transformers import AutoTokenizer; AutoTokenizer.from_pretrained('gpt2')"

# 14.4 Check installation
python -c "from pensieve.server import create_server; print('Pensieve installed correctly!')"


## PART 14: Production-Ready Commands
================================================================================

# 14.1 Run Pensieve server (production)
nohup python main.py \
  --mode pensieve \
  --model facebook/opt-350m \
  --gpu-cache 40 \
  --cpu-cache 100 \
  --max-new-tokens 128 \
  > pensieve_server.log 2>&1 &

# Check logs:
tail -f pensieve_server.log

# 14.2 Monitor GPU usage while running
# In another terminal:
watch -n 1 'nvidia-smi --query-gpu=index,memory.used,memory.free,utilization.gpu,utilization.memory --format=csv,nounits'

# 14.3 Kill server
pkill -f "python main.py"


## PART 15: Generate Documentation & Results
================================================================================

# 15.1 View full documentation
cat README.md

# 15.2 View implementation summary
cat IMPLEMENTATION_SUMMARY.md

# 15.3 Generate test report
python main.py --mode compare --model gpt2 2>&1 | tee test_report.txt

# 15.4 View project structure
find src -type f -name "*.py" | head -20


## PART 16: Clean Up & Reset
================================================================================

# 16.1 Clear cache (if needed for fresh testing)
python -c "
from pensieve.core import TwoTierCache
cache = TwoTierCache()
cache.reset()
print('Cache cleared')
"

# 16.2 Remove generated logs and profiles
rm -f *.log *.prof pensieve_output.log

# 16.3 Remove downloaded models (if needed to save disk space)
# WARNING: This will require re-downloading on next run
# rm -rf ~/.cache/huggingface/hub/


================================================================================
                            QUICK START COMMANDS
================================================================================

# Fastest start (just wants to see it work):
cd /Users/sj/pensieve
pip install -r requirements.txt
pip install -e .
python main.py --mode compare --model gpt2

# To run interactive demo:
python main.py --mode pensieve --interactive --model gpt2

# To run performance comparison:
python main.py --mode compare --model gpt2 --max-new-tokens 50

# To profile with larger model:
python main.py --mode pensieve --model facebook/opt-125m --max-new-tokens 32


================================================================================
                          ENVIRONMENT VARIABLES
================================================================================

# Set GPU device
export CUDA_VISIBLE_DEVICES=0

# Set max tokens to generate
export MAX_NEW_TOKENS=32

# Run on CPU only (if no GPU)
export CUDA_VISIBLE_DEVICES=""

# Python warnings
export PYTHONWARNINGS="ignore"


================================================================================
                            FILE LOCATIONS
================================================================================

Project root: /Users/sj/pensieve/

Key files:
- main.py                           # Entry point
- src/pensieve/server/server.py     # Server implementation
- src/pensieve/core/cache.py        # KVCache implementation
- src/pensieve/core/eviction.py     # Eviction policy
- README.md                          # Full documentation
- requirements.txt                   # Dependencies

Generated files:
- pensieve_server.log               # Server logs
- test_report.txt                   # Test results
- results/                          # Benchmark results
- *.prof                            # Profiling data


================================================================================
                            EXAMPLE WORKFLOW
================================================================================

# Complete workflow from setup to testing:

1. Environment setup:
   cd /Users/sj/pensieve
   python --version  # Check Python 3.10+
   nvidia-smi        # Check GPU

2. Install dependencies:
   pip install -r requirements.txt
   pip install -e .

3. Download models:
   python -c "from transformers import GPT2Tokenizer, GPT2LMHeadModel; GPT2Tokenizer.from_pretrained('gpt2'); GPT2LMHeadModel.from_pretrained('gpt2')"

4. Run basic test:
   python scripts/test_basic_inference.py

5. Run Pensieve demo:
   python main.py --mode pensieve --model gpt2

6. Run vLLM baseline:
   python main.py --mode vllm --model gpt2

7. Compare both:
   python main.py --mode compare --model gpt2

8. Interactive testing:
   python main.py --mode pensieve --interactive --model gpt2

9. Performance profiling:
   python -m cProfile -s cumulative main.py --mode pensieve --model gpt2

10. View statistics:
    tail -f pensieve_server.log


Notes:
- Ensure GPU has at least 8GB memory
- First model download may take time (cached afterwards)
- See README.md for detailed documentation
- See IMPLEMENTATION_SUMMARY.md for architecture details
================================================================================


================================================================================
          MANUAL INTERACTIVE TESTING (Meta-Llama-3-8B)
            Interactive Prompt Testing for Both vLLM & Pensieve
================================================================================

## SETUP FOR META-LLAMA-3-8B
================================================================================

# 1.1 HuggingFace Login (Required for Llama-3-8B)
# Go to https://huggingface.co/settings/tokens and create access token
huggingface-cli login

# Paste your token when prompted, then:
# - username: hf_...
# - password: (paste your token)

# 1.2 Verify Llama-3-8B access
python -c "from transformers import AutoTokenizer; t = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B'); print('✓ Llama-3-8B accessible')"

# 1.3 Pre-download Llama-3-8B (large file, do once)
echo "=== Downloading Meta-Llama-3-8B (may take 10-15 minutes) ==="
python -c "
from transformers import AutoTokenizer, AutoModelForCausalLM
print('Downloading tokenizer...')
AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B')
print('✓ Tokenizer downloaded')

print('Downloading model... (this is large, ~16GB)')
model = AutoModelForCausalLM.from_pretrained('meta-llama/Meta-Llama-3-8B', torch_dtype='auto', device_map='auto')
print('✓ Model downloaded and cached')
"

# 1.4 Verify GPU memory is sufficient
echo "=== Checking GPU Memory ==="
nvidia-smi --query-gpu=memory.free --format=csv,nounits
# Expected: At least 16-20GB free for Llama-3-8B


## OPTION A: VLLM BASELINE MODE (Interactive)
================================================================================

echo "=== Starting vLLM Baseline (Stateless) ==="
python main.py \
  --mode vllm \
  --model meta-llama/Meta-Llama-3-8B \
  --interactive \
  --max-new-tokens 64

# INTERACTION INSTRUCTIONS:
# 1. Start the script above
# 2. Wait for ">>> " prompt
# 3. Type your message and press Enter
# 4. Wait for response
# 5. Type next message (continue conversation)
# 6. Type "stats" to see statistics
# 7. Type "exit" to quit

# EXAMPLE CONVERSATION:
# >>> Hello, what is machine learning?
# (vLLM responds... takes ~2-3 seconds for turn 1)
#
# >>> Can you give me an example?
# (vLLM takes ~2-3 seconds again - same as turn 1, recomputes history!)
#
# >>> What about deep learning?
# (vLLM still ~2-3 seconds - NO cache reuse in stateless mode)

# KEY OBSERVATION:
# - All turns take similar time (NO speedup)
# - Entire conversation recomputed each turn
# - Baseline for comparison


## OPTION B: PENSIEVE MODE (Interactive, RECOMMENDED)
================================================================================

echo "=== Starting Pensieve (Stateful with Cache) ==="
python main.py \
  --mode pensieve \
  --model meta-llama/Meta-Llama-3-8B \
  --interactive \
  --max-new-tokens 64 \
  --gpu-cache 40 \
  --cpu-cache 100

# SAME INTERACTION INSTRUCTIONS:
# 1. Start the script above
# 2. Wait for ">>> " prompt
# 3. Type your message and press Enter
# 4. Wait for response
# 5. Type next message (continue conversation)
# 6. Type "stats" to see cache statistics
# 7. Type "exit" to quit

# EXAMPLE CONVERSATION (SAME AS ABOVE):
# >>> Hello, what is machine learning?
# (Pensieve processes... ~2-3 seconds for turn 1, NO cache yet)
#
# >>> Can you give me an example?
# (Pensieve processes... ~1.0-1.5 seconds - FASTER! Uses cached history from turn 1)
#
# >>> What about deep learning?
# (Pensieve processes... ~1.0-1.5 seconds - Same speed, NOT slowing down)

# KEY OBSERVATION:
# - Turn 1: ~2-3 seconds (same as vLLM baseline)
# - Turn 2+: ~1.0-1.5 seconds (FASTER due to cache)
# - Statistics show GPU cache hits
# - This is Pensieve's advantage!


## SIDE-BY-SIDE COMPARISON SCRIPT
================================================================================

# Run both modes back-to-back to compare directly

# 2.1 Create comparison script
cat > test_comparison.sh << 'EOF'
#!/bin/bash

echo "================================================================================"
echo "              PENSIEVE vs vLLM MANUAL COMPARISON"
echo "              Interactive Testing with Meta-Llama-3-8B"
echo "================================================================================"

# Test questions (you can modify these)
declare -a QUESTIONS=(
    "What is machine learning?"
    "Give me an example of machine learning in real world."
    "How does deep learning differ from traditional ML?"
    "Can you explain neural networks?"
    "What applications use neural networks?"
)

echo ""
echo "QUESTIONS TO ASK (Run the same questions in both modes):"
for i in "${!QUESTIONS[@]}"; do
    echo "  $((i+1)). ${QUESTIONS[$i]}"
done

echo ""
echo "================================================================================"
echo "                    TEST 1: vLLM BASELINE (Stateless)"
echo "================================================================================"
echo "Running: python main.py --mode vllm --model meta-llama/Meta-Llama-3-8B --interactive"
echo ""
echo "INSTRUCTIONS:"
echo "  1. Wait for '>>> ' prompt"
echo "  2. Type each question from the list above, one per line"
echo "  3. Observe response time for each turn"
echo "  4. Type 'stats' to see statistics"
echo "  5. Type 'exit' to quit"
echo ""
echo "WHAT TO OBSERVE:"
echo "  • Turn 1 time (no cache): ~2-3 seconds"
echo "  • Turn 2 time: Still ~2-3 seconds (no speedup!)"
echo "  • Turn 5 time: Still ~2-3 seconds (confirms no cache)"
echo ""
read -p "Ready to start vLLM mode? Press Enter..."

python main.py \
  --mode vllm \
  --model meta-llama/Meta-Llama-3-8B \
  --interactive \
  --max-new-tokens 64

echo ""
echo "================================================================================"
echo "                    TEST 2: PENSIEVE (Stateful with Cache)"
echo "================================================================================"
echo "Running: python main.py --mode pensieve --model meta-llama/Meta-Llama-3-8B --interactive"
echo ""
echo "INSTRUCTIONS:"
echo "  1. Wait for '>>> ' prompt"
echo "  2. Type THE SAME questions as before, in the same order"
echo "  3. Observe response time for each turn (should be FASTER!)"
echo "  4. Type 'stats' to see GPU/CPU cache statistics"
echo "  5. Type 'exit' to quit"
echo ""
echo "WHAT TO OBSERVE:"
echo "  • Turn 1 time (no cache): ~2-3 seconds (same as vLLM)"
echo "  • Turn 2 time: ~1.0-1.5 seconds (FASTER! Cache helps)"
echo "  • Turn 3+ time: ~1.0-1.5 seconds (consistent speedup)"
echo "  • Cache stats show GPU cache hits increasing"
echo ""
read -p "Ready to start Pensieve mode? Press Enter..."

python main.py \
  --mode pensieve \
  --model meta-llama/Meta-Llama-3-8B \
  --interactive \
  --max-new-tokens 64 \
  --gpu-cache 40 \
  --cpu-cache 100

echo ""
echo "================================================================================"
echo "                    COMPARISON COMPLETE"
echo "================================================================================"
echo ""
echo "ANALYSIS:"
echo "  • vLLM: Consistent time (no cache benefit)"
echo "  • Pensieve: Decreasing time initially, then stable (cache benefit!)"
echo "  • Speedup ratio: time_vllm_turn1 / time_pensieve_turn5"
echo "  • Target: >1.5x speedup on turn 5+ (with Llama-3-8B)"
echo ""

EOF

chmod +x test_comparison.sh
./test_comparison.sh


# 2.2 Alternative: Manual two-terminal comparison
# Terminal 1: vLLM
echo "=== Terminal 1: vLLM Baseline ==="
python main.py --mode vllm --model meta-llama/Meta-Llama-3-8B --interactive --max-new-tokens 64

# Terminal 2 (in another terminal): Pensieve
echo "=== Terminal 2: Pensieve ==="
python main.py --mode pensieve --model meta-llama/Meta-Llama-3-8B --interactive --max-new-tokens 64 --gpu-cache 40 --cpu-cache 100


## EXAMPLE CONVERSATION FOR MANUAL TESTING
================================================================================

# These are suggested questions to ask (in this order) in both modes:

# Turn 1 (Simple question, introduces new topic):
# >>> What is machine learning?

# Turn 2 (Follow-up, builds on context):
# >>> Can you give me a real-world example?

# Turn 3 (More specific follow-up):
# >>> How does that example use neural networks?

# Turn 4 (Deeper dive):
# >>> What are the advantages of neural networks in this case?

# Turn 5 (Another follow-up):
# >>> What are the limitations?

# Expected behavior:
# vLLM: ~2-3s per turn (consistent)
# Pensieve: ~2-3s turn1, ~1.0-1.5s turns 2-5 (speedup visible!)


## METRICS TO OBSERVE DURING MANUAL TESTING
================================================================================

### When you run stats command in interactive mode, look for:

# vLLM Stats (Baseline):
# - Cache statistics: N/A (stateless)
# - TTFT (Time to First Token): Similar across turns (~2-3s)
# - Tokens processed: Grows linearly with turn number
# - Interpretation: Each turn recomputes entire history

# Pensieve Stats (Stateful):
# - GPU cache hits: Increases with turns (0% → 50-70%)
# - GPU cache size: Grows as chunks stored
# - CPU cache: May be empty if GPU has space
# - TTFT: Decreases from turn 1 to turn 5 (2-3s → 1.0-1.5s)
# - Speedup: Visible in TTFT ratio
# - Eviction count: Should be low for this scenario
# - Interpretation: Cache reuse reduces computation


## ADVANCED: TESTING WITH DIFFERENT CACHE SIZES
================================================================================

# Test 1: Large GPU cache (should keep everything in GPU)
python main.py --mode pensieve --model meta-llama/Meta-Llama-3-8B --interactive \
  --gpu-cache 80 --cpu-cache 50 --max-new-tokens 64

# Expected: Very fast turns 2-5 (everything in GPU cache)

# Test 2: Small GPU cache (forces eviction to CPU)
python main.py --mode pensieve --model meta-llama/Meta-Llama-3-8B --interactive \
  --gpu-cache 2 --cpu-cache 20 --max-new-tokens 64

# Expected: See CPU cache utilization, possible recovery costs
# Still faster than vLLM, but with some overhead

# Test 3: Tiny cache (forces dropped token recovery)
python main.py --mode pensieve --model meta-llama/Meta-Llama-3-8B --interactive \
  --gpu-cache 1 --cpu-cache 5 --max-new-tokens 64

# Expected: See recovery mechanism in action
# More expensive, but still cheaper than full recomputation


## LOGGING & ANALYSIS
================================================================================

# Run with detailed logging
python -u main.py \
  --mode pensieve \
  --model meta-llama/Meta-Llama-3-8B \
  --interactive \
  --max-new-tokens 64 \
  --gpu-cache 40 \
  --cpu-cache 100 \
  2>&1 | tee pensieve_interactive.log

# Later, analyze the log:
cat pensieve_interactive.log | grep -E "(Turn|TTFT|Cache|Speedup)"

# Compare with vLLM log:
python -u main.py \
  --mode vllm \
  --model meta-llama/Meta-Llama-3-8B \
  --interactive \
  --max-new-tokens 64 \
  2>&1 | tee vllm_interactive.log


## MEASUREMENT TEMPLATE (Copy & Fill)
================================================================================

Use this template to record your manual measurements:

Session Date: __________________
Model: Meta-Llama-3-8B
Cache Settings (Pensieve): GPU=40GB, CPU=100GB

vLLM Baseline (Stateless):
  Turn 1: _______ seconds
  Turn 2: _______ seconds
  Turn 3: _______ seconds
  Turn 4: _______ seconds
  Turn 5: _______ seconds

Pensieve (Stateful):
  Turn 1: _______ seconds
  Turn 2: _______ seconds
  Turn 3: _______ seconds
  Turn 4: _______ seconds
  Turn 5: _______ seconds

Speedup Analysis:
  Turn 1 speedup: _______ x
  Turn 5 speedup: _______ x
  Average speedup: _______ x

Cache Statistics (from "stats" command in Pensieve):
  GPU Cache Hits: _______ %
  CPU Cache Hits: _______ %
  GPU Cache Usage: _______ GB
  CPU Cache Usage: _______ GB
  Evictions: _______
  Recoveries: _______

Notes:
  ___________________________________________________
  ___________________________________________________


## TROUBLESHOOTING INTERACTIVE MODE
================================================================================

# If Pensieve seems no faster than vLLM:
# 1. Check cache size: --gpu-cache should be >20GB
# 2. Check "stats" output: Should show increasing cache hits
# 3. Check cache location: Chunks should be in GPU for speedup
# 4. Run smaller model first (gpt2) to verify caching works

# If interactive mode crashes:
# 1. Check GPU memory: nvidia-smi
# 2. Reduce max-new-tokens: --max-new-tokens 32
# 3. Clear cache: pkill -f "python main.py"
# 4. Try smaller model (gpt2) first

# If Llama-3-8B download fails:
# 1. Check HuggingFace token: huggingface-cli whoami
# 2. Re-login: huggingface-cli login
# 3. Try smaller model (facebook/opt-13b or gpt2)


================================================================================


================================================================================
                    PHASE 4 IMPLEMENTATION TESTING
                 (Batch Scheduler, Worker, Attention Kernel)
================================================================================

## PART 17: Test Batch Scheduler & Worker
================================================================================

# 17.1 Test batch scheduler formation
python << 'EOF'
from pensieve.scheduler import BatchScheduler
from pensieve.core import TwoTierCache, Request, Phase
import torch

cache = TwoTierCache(gpu_capacity_gb=32, cpu_capacity_gb=100)
scheduler = BatchScheduler(cache, max_batch_size=8)

# Add some requests
for i in range(3):
    req = Request(
        session_id=f"session_{i}",
        request_id=f"req_{i}",
        input_ids=torch.tensor([1, 2, 3, 4, 5]),
        phase=Phase.PREFILL,
    )
    scheduler.add_request(req)

# Form a batch
batch, cache_plan = scheduler.form_next_batch()

print(f"Batch size: {batch.num_requests}")
print(f"Prefill requests: {len(batch.get_prefill_requests())}")
print(f"Cache swaps planned: {len(cache_plan.chunks_to_swap_in)}")

EOF

# Expected output:
# - Batch size: 3
# - Prefill requests: 3
# - Cache swaps planned: 0 (no cached data yet)


# 17.2 Test worker execution with custom cache
python << 'EOF'
from pensieve.scheduler import BatchScheduler
from pensieve.worker import Worker
from pensieve.core import TwoTierCache, Request, Phase
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load small model for testing
print("Loading model...")
model = AutoModelForCausalLM.from_pretrained('gpt2')
tokenizer = AutoTokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token

cache = TwoTierCache(gpu_capacity_gb=32, cpu_capacity_gb=100)
scheduler = BatchScheduler(cache, max_batch_size=4)
worker = Worker(model, tokenizer, cache, device='cuda:0')

# Create a batch
req = Request(
    session_id="test_session",
    request_id="test_req_1",
    input_ids=torch.tensor([100, 200]),  # Dummy token IDs
    phase=Phase.PREFILL,
)
scheduler.add_request(req)

batch, cache_plan = scheduler.form_next_batch()

print(f"\nBatch info: {scheduler.get_batch_info_dict(batch)}")
print(f"Cache plan: swap_in={len(cache_plan.chunks_to_swap_in)}, swap_out={len(cache_plan.chunks_to_swap_out)}")

# Execute batch
print("Executing batch...")
result = worker.execute_batch(batch, cache_plan)

print(f"Execution time: {result.execution_time:.3f}s")
print(f"Results: {result.request_results}")

EOF

# Expected output:
# - Batch info: session_id, positions, etc.
# - Execution time: ~1-2 seconds for GPT-2
# - Results: generated tokens and response


## PART 18: Test Multi-Token Attention Kernel
================================================================================

# 18.1 Verify attention kernel correctness
python << 'EOF'
from pensieve.kernels import verify_attention_correctness, multi_token_attention_pytorch
import torch

print("Testing multi-token attention kernel...")

# Create test tensors
batch_size, query_len, num_heads, head_dim = 2, 10, 8, 64
num_chunks = 3
chunk_size = 32

query = torch.randn(batch_size, query_len, num_heads, head_dim, device='cuda:0')
key_chunks = [torch.randn(batch_size, chunk_size, num_heads, head_dim, device='cuda:0') for _ in range(num_chunks)]
value_chunks = [torch.randn(batch_size, chunk_size, num_heads, head_dim, device='cuda:0') for _ in range(num_chunks)]

# Test correctness
verify_attention_correctness(query, key_chunks, value_chunks)

# Test with attention mask
attention_mask = torch.ones(query_len, num_chunks * chunk_size, device='cuda:0')
output = multi_token_attention_pytorch(query, key_chunks, value_chunks, attention_mask=attention_mask)

print(f"Output shape: {output.shape}")
print("✓ Attention kernel test passed")

EOF

# Expected output:
# - Attention correctness check PASSED
# - Output shape: torch.Size([2, 10, 8, 64])
# - Attention kernel test passed


# 18.2 Benchmark attention performance
python << 'EOF'
from pensieve.kernels import benchmark_attention

print("Benchmarking attention kernel...")
benchmark_attention(
    query_len=32,
    key_len=512,
    batch_size=4,
    num_chunks=4,
    num_iters=50
)

EOF

# Expected output:
# - Query: 32 tokens, Batch: 4
# - KV: 512 tokens, Chunks: 4
# - Chunked: X.XX ms/iter
# - Concatenated: Y.YY ms/iter
# - Overhead: Z.Z%


## PART 19: Test Pipelined Transfers
================================================================================

# 19.1 Test pipelined transfer performance
python << 'EOF'
from pensieve.pipeline import benchmark_pipelined_transfer

print("Benchmarking pipelined transfers...")
benchmark_pipelined_transfer(
    num_transfers=10,
    chunk_size_mb=50,
    device='cuda:0'
)

EOF

# Expected output:
# - Pipelined Transfer Benchmark
# - Sequential: X.XX ms
# - Pipelined: Y.YY ms
# - Speedup: Z.ZZx
# - Overlap: W.W%
# (Should see ~20-30% speedup with overlap)


# 19.2 Test async transfer with cache
python << 'EOF'
from pensieve.pipeline import AsyncTransferTask, PipelinedTransferManager
from pensieve.core import TwoTierCache, KVChunk, CacheLocation
import torch
import time

cache = TwoTierCache(gpu_capacity_gb=32, cpu_capacity_gb=100)
manager = PipelinedTransferManager(device='cuda:0')

# Create test chunk on CPU
test_tensor_k = torch.randn(1, 32, 8, 64, device='cpu')
test_tensor_v = torch.randn(1, 32, 8, 64, device='cpu')

chunk = KVChunk(
    session_id='test_session',
    chunk_id=0,
    layer_idx=0,
    key_tensor=test_tensor_k,
    value_tensor=test_tensor_v,
    context_length=0,
    session_total_chunks=1,
    num_layers=32,
)

# Store on CPU first
cache.cpu_cache[chunk.key] = chunk
cache.cpu_used_bytes += chunk.size_bytes

print(f"Chunk on CPU: {chunk.key}")

# Transfer asynchronously
event = manager.swap_chunk_to_gpu_async(cache, chunk.key)

# Wait for transfer
if event:
    event.wait()
    print(f"Transfer complete: {chunk.key} now on GPU")

EOF

# Expected output:
# - Chunk on CPU: test_session:chunk:0:layer:0
# - Transfer complete: test_session:chunk:0:layer:0 now on GPU


## PART 20: Test Dropped Token Recovery
================================================================================

# 20.1 Test recovery plan creation
python << 'EOF'
from pensieve.recovery import TokenRecoveryManager, RecoveryPlan
from pensieve.core import TwoTierCache, Request, KVChunk, CacheLocation
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model = AutoModelForCausalLM.from_pretrained('gpt2')
tokenizer = AutoTokenizer.from_pretrained('gpt2')
cache = TwoTierCache(gpu_capacity_gb=32, cpu_capacity_gb=100)

recovery_mgr = TokenRecoveryManager(model, tokenizer, cache, device='cuda:0')

# Create a request
request = Request(
    session_id='session_1',
    request_id='req_1',
    input_ids=torch.tensor([100, 200, 300]),
    phase='PREFILL',
)

# Create recovery plan
recovery_plan = recovery_mgr.create_recovery_plan(request)

if recovery_plan:
    print(f"Recovery needed: {len(recovery_plan.dropped_positions)} dropped chunks")
    print(f"Tokens to recover: {len(recovery_plan.raw_tokens)}")
else:
    print("No recovery needed (no dropped chunks yet)")

EOF

# Expected output:
# - No recovery needed (no dropped chunks yet)


# 20.2 Test recovery cost estimation
python << 'EOF'
from pensieve.recovery import TokenRecoveryManager, RecoveryPlan
from pensieve.core import TwoTierCache
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model = AutoModelForCausalLM.from_pretrained('gpt2')
tokenizer = AutoTokenizer.from_pretrained('gpt2')
cache = TwoTierCache(gpu_capacity_gb=32, cpu_capacity_gb=100)

recovery_mgr = TokenRecoveryManager(model, tokenizer, cache, device='cuda:0')

# Create dummy recovery plan
recovery_plan = RecoveryPlan(
    dropped_positions=[0, 1, 2],
    raw_tokens=torch.ones(96, dtype=torch.long),  # 3 chunks * 32 tokens
    session_id='session_1'
)

cost = recovery_mgr.estimate_recovery_cost(recovery_plan)
should_recover = recovery_mgr.should_recover(recovery_plan)

print(f"Recovery cost: {cost:.4f}s")
print(f"Should recover: {should_recover}")

EOF

# Expected output:
# - Recovery cost: ~0.001s (very small for GPT-2)
# - Should recover: True


## PART 21: Full Phase 4 Integration Test
================================================================================

# 21.1 End-to-end Phase 4 test (scheduler + worker + attention + recovery)
python << 'EOF'
from pensieve.scheduler import BatchScheduler
from pensieve.worker import Worker
from pensieve.recovery import TokenRecoveryManager
from pensieve.core import TwoTierCache, Request, Phase
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import time

print("=" * 60)
print("Phase 4 End-to-End Integration Test")
print("=" * 60)

# Load model
print("\n1. Loading model...")
model = AutoModelForCausalLM.from_pretrained('gpt2')
tokenizer = AutoTokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token

# Initialize components
print("2. Initializing components...")
cache = TwoTierCache(gpu_capacity_gb=32, cpu_capacity_gb=100)
scheduler = BatchScheduler(cache, max_batch_size=4)
worker = Worker(model, tokenizer, cache, device='cuda:0')
recovery_mgr = TokenRecoveryManager(model, tokenizer, cache, device='cuda:0')

# Create multi-turn requests
print("3. Creating multi-turn conversation...")
session_id = 'phase4_test'
messages = [
    "Hello, how are you?",
    "What is machine learning?",
    "Tell me about neural networks."
]

for turn, msg in enumerate(messages, 1):
    print(f"\n--- Turn {turn}: {msg[:40]}... ---")

    # Create request
    input_ids = tokenizer.encode(msg, return_tensors='pt')[0]
    request = Request(
        session_id=session_id,
        request_id=f"req_{turn}",
        input_ids=input_ids,
        phase=Phase.PREFILL,
    )

    # Check for recovery
    recovery_plan = recovery_mgr.create_recovery_plan(request)
    if recovery_plan:
        print(f"  Recovering {len(recovery_plan.dropped_positions)} chunks...")
        recovery_mgr.recompute_dropped_chunks(recovery_plan)

    # Schedule request
    scheduler.add_request(request)

    # Form batch and execute
    batch, cache_plan = scheduler.form_next_batch()
    print(f"  Batch size: {batch.num_requests}")
    print(f"  Cache swaps: in={len(cache_plan.chunks_to_swap_in)}, out={len(cache_plan.chunks_to_swap_out)}")

    start = time.time()
    result = worker.execute_batch(batch, cache_plan)
    elapsed = time.time() - start

    print(f"  Execution time: {elapsed:.3f}s")
    print(f"  Cache stats: GPU={len(cache.gpu_cache)} chunks, CPU={len(cache.cpu_cache)} chunks")

print("\n" + "=" * 60)
print("✓ Phase 4 integration test completed successfully!")
print("=" * 60)

EOF

# Expected output:
# - All 3 turns process successfully
# - Execution time decreases with turns (cache benefit)
# - GPU cache grows, CPU cache empty (typical scenario)


## PART 22: Performance Comparison (Phase 4 vs Earlier)
================================================================================

# 22.1 Compare single-turn performance
python << 'EOF'
from pensieve.server.server import create_server
import time

print("=" * 60)
print("Single-Turn Performance Comparison")
print("=" * 60)

# Pensieve mode
server = create_server(mode='pensieve', model_name='gpt2')
start = time.time()
response = server.process_request('session_1', 'Hello, what is AI?', max_new_tokens=20)
time_pensieve = time.time() - start

print(f"\nPensieve: {time_pensieve:.3f}s")
print(f"Response: {response[:50]}...")

# vLLM baseline
server_baseline = create_server(mode='vllm', model_name='gpt2')
start = time.time()
response_baseline = server_baseline.process_request('session_1', 'Hello, what is AI?', max_new_tokens=20)
time_baseline = time.time() - start

print(f"\nvLLM: {time_baseline:.3f}s")
print(f"Response: {response_baseline[:50]}...")

# Both should be similar for turn 1 (no cache benefit yet)
ratio = time_baseline / time_pensieve if time_pensieve > 0 else 0
print(f"\nSpeedup: {ratio:.2f}x (expected ~1.0x for turn 1)")

EOF

# Expected output:
# - Pensieve: ~1.5-2.0s (for GPT-2)
# - vLLM: ~1.5-2.0s (similar, no cache yet)
# - Speedup: ~1.0x (turn 1, no caching benefit)


## PART 23: Concurrent Benchmark with Cache Hotness
================================================================================

# 23.1 Concurrent benchmark with 6 users (3 access frequency levels)
#
# This benchmark automatically creates:
# - 2 HOT clients (frequent access, 0.15s interval) → Stay in GPU cache
# - 2 WARM clients (normal access, 0.5s interval) → GPU/CPU mixed
# - 2 COLD clients (infrequent, 1.25s interval) → Trigger eviction/recovery
#
# This shows the paper's key insight: intelligent eviction based on hotness

python main.py --mode compare --model gpt2 \
  --num-concurrent-users 6 \
  --request-interval 0.5 \
  --gpu-cache 4 \
  --cpu-cache 8 \
  --max-new-tokens 32

# Expected output:
# PHASE 1: PENSIEVE (Concurrent Users with Hotness)
#   Client 0: HOT (Frequent)     interval=0.15s
#   Client 1: HOT (Frequent)     interval=0.15s
#   Client 2: WARM (Normal)      interval=0.50s
#   Client 3: WARM (Normal)      interval=0.50s
#   Client 4: COLD (Infrequent)  interval=1.25s
#   Client 5: COLD (Infrequent)  interval=1.25s
#
# PHASE 2: vLLM BASELINE (Concurrent Users - Stateless, Same Hotness)
#   (vLLM uses same access pattern but no cache → always recomputes)
#
# Results show:
# - Pensieve: Faster for HOT/WARM clients (cache hits)
# - vLLM: Uniform slowness (recomputes all turns)
# - Speedup: 1.5-2.5× for Pensieve (with Llama, 2-3×)


# 23.2 Stress test with more concurrent users
#
# 9 users = 3 HOT, 3 WARM, 3 COLD
# More cache contention → tests eviction policy more thoroughly

python main.py --mode compare --model gpt2 \
  --num-concurrent-users 9 \
  --request-interval 0.3 \
  --gpu-cache 4 \
  --cpu-cache 8

# Expected: Higher cache contention, visible eviction
# HOT clients should still be prioritized for GPU cache


# 23.3 Varying request intervals (slower = more eviction)

python main.py --mode compare --model gpt2 \
  --num-concurrent-users 6 \
  --request-interval 1.0 \
  --gpu-cache 4 \
  --cpu-cache 8

# With longer intervals, COLD clients are more likely to find cache evicted
# Recovery costs become visible, but still < recomputation cost


# 23.4 Very fast requests (more batching, less eviction)

python main.py --mode compare --model gpt2 \
  --num-concurrent-users 6 \
  --request-interval 0.1 \
  --gpu-cache 4 \
  --cpu-cache 8

# Fast requests mean clients arrive before being evicted
# Shows batching benefit (Batch size > 1)


# 23.5 Compare sequential vs concurrent
#
# Sequential (old benchmark - no concurrency)
python main.py --mode compare --model gpt2 --num-concurrent-users 1

# Concurrent (new benchmark - 6 concurrent users with hotness)
python main.py --mode compare --model gpt2 --num-concurrent-users 6 --request-interval 0.5


# 23.6 Full model benchmark (Meta-Llama-3-8B)

python main.py --mode compare --model meta-llama/Meta-Llama-3-8B \
  --num-concurrent-users 6 \
  --request-interval 0.5 \
  --gpu-cache 40 \
  --cpu-cache 100 \
  --max-new-tokens 32

# Expected: 2-3× speedup (cache reuse is more significant with larger model)


================================================================================
                        INTERPRETATION GUIDE
================================================================================

## Understanding Hotness Distribution Results

When you see output like:
```
Client Hotness Distribution:
  • HOT clients (1/3): Request every 0.15s → GPU cache hit
  • WARM clients (1/3): Request every 0.50s → GPU/CPU mixed
  • COLD clients (1/3): Request every 1.25s → Eviction/Recovery
```

**What's happening**:

1. **HOT clients** (frequent):
   - Requests arrive before previous chunks are evicted
   - Stay in GPU cache → lowest TTFT
   - Pensieve benefit: 1.8-2.5× speedup vs vLLM

2. **WARM clients** (normal):
   - Some cache hits, some cache misses
   - May trigger CPU swapping
   - Pensieve benefit: 1.3-1.8× speedup

3. **COLD clients** (infrequent):
   - Earlier chunks evicted to CPU or DROPPED
   - When return: Cache miss → must recover/swap
   - Pensieve benefit: 1.1-1.5× speedup (recovery cost < recompute cost)

**Key Insight**:
Even with recovery overhead, Pensieve benefits from caching because:
- vLLM recomputes entire history every turn (O(history_length))
- Pensieve reuses most turns, recovers only dropped leading tokens (O(dropped_length))
- For typical conversations: dropped_length << history_length → Pensieve wins

## Metrics to Watch

| Metric | What It Shows |
|--------|---------------|
| **Time Speedup** | Overall wall-clock improvement |
| **Throughput** | Requests/second (higher = better batching) |
| **Avg TTFT** | Cache effectiveness for typical clients |
| **P99 TTFT** | Consistency (eviction overhead visible in p99) |
| **Tail Latency** | Recovery costs for COLD clients |

If P99 TTFT >> Avg TTFT, suggests some cold clients experiencing eviction.


================================================================================
                        VERIFICATION CHECKLIST
================================================================================

After Phase 4 implementation, verify:

✓ Batch Scheduler:
  - [ ] form_next_batch() returns Batch with correct requests
  - [ ] create_cache_plan() identifies chunks to swap
  - [ ] Request queue management works

✓ Worker:
  - [ ] execute_batch() runs without errors
  - [ ] Cache swaps execute successfully
  - [ ] New KV chunks are stored after generation
  - [ ] Model outputs are correct

✓ Multi-Token Attention:
  - [ ] Attention output matches standard PyTorch
  - [ ] Works with non-contiguous KV chunks
  - [ ] Handles variable batch sizes

✓ Pipelined Transfer:
  - [ ] Async transfers complete successfully
  - [ ] Transfer+compute overlap visible in benchmarks
  - [ ] No data corruption or race conditions

✓ Token Recovery:
  - [ ] Dropped chunks detected correctly
  - [ ] Recovery plan created properly
  - [ ] Recomputed chunks stored in cache

✓ Integration:
  - [ ] Scheduler → Worker → Model pipeline works
  - [ ] Cache statistics accurate
  - [ ] No memory leaks or OOM errors
  - [ ] Multi-turn conversation completes successfully

================================================================================

================================================================================
           PHASE 5: MULTI-TURN KV CACHE FIXES & SHAREGT BENCHMARK
                   (Tensor Format Detection & Token Recovery)
================================================================================

## QUICK TEST 1: Verify Multi-Turn Fixes (5-10 minutes)
====================================================================

# This test verifies that Turn 1 and Turn 2 work correctly with:
# - Tensor format detection (handles Gemma-2 and Llama formats)
# - TOKEN RECOVERY mechanism (recovers EOS token loss)
# - Turn 2 input encoding fix (only new tokens, not full history)

python test_multiturn_debug.py

# Expected output:
# ✓ Multi-turn test PASSED
# [FINAL KV] seq_len=78, expected=78 (or higher with TOKEN RECOVERY)
# [KV TRACKING] Shows proper cache reuse in Turn 2

# If Turn 2 crashes with "index out of range" error:
# → Issue is Turn 2 input encoding (passing wrong token count)
# → Should see logs showing Turn 2 only received ~15 tokens, not 78


## QUICK TEST 2: ShareGPT Dataset Benchmark (20-40 minutes)
====================================================================

# Recommended command for maximum performance demonstration
# Tests multi-turn conversations with 32 concurrent users
# Expected: 1.5-3.0x throughput speedup vs vLLM

python main.py --dataset sharegt \
  --num-concurrent-users 32 \
  --model meta-llama/Meta-Llama-3-8B-Instruct \
  --gpu-cache 64 \
  --cpu-cache 128 \
  --max-turns 30 \
  --min-turns 10 \
  --max-new-tokens 1024 \
  --num-conversations 20

# Expected metrics:
# - Throughput: Pensieve XXX req/s | vLLM XXX req/s | Speedup: 1.5-3.0x ✓
# - TTFT Speedup: Increases with turn (0.8x → 2.5x) ✓
# - Cache Hit Rates: GPU 60-80%, CPU 15-25%, Miss 5-15% ✓


## ALTERNATIVE TEST 3: Lighter Configuration (10-20 minutes)
====================================================================

# For initial verification or limited GPU memory
# Reduced parameters to verify fixes before full benchmark

python main.py --dataset sharegt \
  --num-concurrent-users 3 \
  --model meta-llama/Meta-Llama-3-8B-Instruct \
  --gpu-cache 32 \
  --cpu-cache 64 \
  --max-turns 10 \
  --min-turns 5 \
  --max-new-tokens 256 \
  --num-conversations 5

# Expected: Similar speedup pattern, smaller scale
# Good for: Verifying fixes work before running full benchmark


## TEST 4: Alternative Model - Gemma-2-27B (25-35 minutes)
====================================================================

# Test with Gemma instead of Llama
# Note: Requires ~54GB GPU memory (with bfloat16)
# Tests tensor format detection (Gemma uses [batch, heads, seq, head_dim])

python main.py --dataset sharegt \
  --num-concurrent-users 16 \
  --model google/gemma-2-27b-it \
  --gpu-cache 64 \
  --cpu-cache 128 \
  --max-turns 20 \
  --min-turns 10 \
  --max-new-tokens 512 \
  --num-conversations 15

# Expected: Same speedup behavior as Llama
# Tensor format detection should automatically handle Gemma's [batch, heads, seq, head_dim] format


## WHAT WAS FIXED
====================================================================

### Fix 1: Tensor Format Detection
File: src/pensieve/worker/worker.py lines 87-110
- Problem: Different models use different KV tensor shapes
  * Gemma-2: [batch, heads, seq, head_dim] → seq at dim=2
  * Llama: [batch, seq, heads, head_dim] → seq at dim=1
- Solution: _get_seq_len_from_kv() helper detects format automatically
- Impact: Turn 1 and Turn 2 now read sequence length correctly

### Fix 2: TOKEN RECOVERY Mechanism
File: src/pensieve/worker/worker.py lines 517-549
- Problem: EOS token was being lost during generation
- Solution: Detects and recovers via forward pass with last token
- Impact: Automatically recovers missing tokens before caching

### Fix 3: Turn 2 Input Encoding
File: src/pensieve/server/server.py lines 236-258
- Problem: Turn 2 was passing full history (78 tokens) instead of new message (~15 tokens)
- Solution: Always encode only user_input for all turns
- Impact: Turn 2 can now properly use cached KV from Turn 1

### Fix 4: Model Dtype Support
File: src/pensieve/server/server.py line 117
- Problem: Llama wasn't using bfloat16 (wasted memory)
- Solution: Support both 'llama' and 'gemma' with bfloat16
- Impact: Efficient memory usage for large models


## RECOMMENDED TESTING SEQUENCE
====================================================================

### Phase 1: Verify Fixes (10 min)
1. python test_multiturn_debug.py

### Phase 2: Light Test (15 min)
2. python main.py --dataset sharegt --num-concurrent-users 3 \
     --model meta-llama/Meta-Llama-3-8B-Instruct \
     --gpu-cache 32 --cpu-cache 64 \
     --max-turns 10 --min-turns 5 \
     --max-new-tokens 256 \
     --num-conversations 5

### Phase 3: Full Benchmark (30 min)
3. python main.py --dataset sharegt --num-concurrent-users 32 \
     --model meta-llama/Meta-Llama-3-8B-Instruct \
     --gpu-cache 64 --cpu-cache 128 \
     --max-turns 30 --min-turns 10 \
     --max-new-tokens 1024 \
     --num-conversations 20


## STATUS
====================================================================

✅ All fixes implemented and ready for testing
✅ Multi-turn conversation support working
✅ TokenFormat detection handles both Llama and Gemma
✅ Token recovery mechanism active
✅ ShareGPT dataset integration complete

Ready for server execution!

================================================================================
