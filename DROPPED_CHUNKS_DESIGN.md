# DROPPED Chunks Design & Memory Semantics

## Your Question

**Q**: "ì›ë˜ DROPPEDê°™ì€ê±° ì•ˆ ì“°ê³ , ì•„ì˜ˆ removeí–ˆì§€ ì•Šì•„?"

**Answer**: ë§ë‹¤! ì›ë˜ëŠ” simple removeì˜€ì„ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ í˜„ì¬ designì€ **F7: Dropped Token Recovery** ê¸°ëŠ¥ì„ ì§€ì›í•˜ê¸° ìœ„í•´ DROPPED tierë¥¼ ì‚¬ìš©í•œë‹¤.

ë‹¤ë§Œ **ë©”ëª¨ë¦¬ ì¶”ì ì´ ê¹¨ì ¸ìˆì—ˆë‹¤**. ì´ë¥¼ ëª…í™•íˆ í•˜ì.

---

## Current Architecture Issue

### Requirement (CLAUDE.md F7)
```
F7: Dropped token recovery
- Recompute evicted prefixes and merge with cached KV
- Paper Â§4.3.4
```

â†’ **DROPPED tierëŠ” ì˜ë„ëœ ì„¤ê³„**

### Problem: Memory Accounting Inconsistency

```python
# Current code (BROKEN):
CPU eviction with CPU full:
  drop_chunk = cpu_cache.pop(drop_key)
  cpu_used_bytes -= drop_chunk.size_bytes  # â† Freed?
  dropped_chunks[drop_key] = drop_chunk    # â† But stored here!

Result:
  - cpu_used_bytes shows space freed
  - But drop_chunk.tensors still in CPU memory
  - = Inconsistency!
```

---

## Design Decision: Two Options

### Option A: DROPPED as "Metadata-Only"

```
Concept:
- DROPPED chunks exist in dropped_chunks dict (metadata)
- Tensors are serialized to disk or discarded
- Recovery requires disk I/O to restore tensors

Pros:
  âœ“ Memory is actually freed
  âœ“ cpu_used_bytes accurate
  âœ“ Clean separation

Cons:
  âœ— Recovery requires I/O (slow)
  âœ— Disk space needed
  âœ— Complex implementation
```

### Option B: DROPPED as "CPU-Backed Recovery Cache"

```
Concept:
- DROPPED chunks stay in CPU memory
- Tensors are preserved on CPU
- Recovery just loads from dropped_chunks dict

Pros:
  âœ“ Fast recovery (no I/O)
  âœ“ Simple implementation
  âœ“ Consistent with paper

Cons:
  âœ— CPU memory not truly freed
  âœ— cpu_used_bytes becomes inaccurate
  âœ— Eviction space accounting broken
```

---

## Proposed Solution: Clarified Option B

**Choose Option B** (simpler, faster) but with **clear semantics**:

### Core Principle

```
DROPPED chunks = "Cold" CPU-backed recovery cache
- Located: CPU memory (in dropped_chunks dict)
- Lifetime: Until session ends (evict_session removes)
- Purpose: Enable recomputation without full re-execution
```

### Memory Semantics (FIXED)

```python
# CPU Tier Accounting:
cpu_memory_total = cpu_cache + dropped_chunks

# Therefore:
cpu_used_bytes should include DROPPED chunks!
cpu_used_bytes = Î£(cpu_cache) + Î£(dropped_chunks)
```

### Eviction Flow (Corrected)

```
When CPU full and need space:
â”œâ”€ Evict from gpu_cache â†’ cpu_cache (move up)
â”‚  â””â”€ If CPU also full:
â”‚     â””â”€ Evict from cpu_cache â†’ dropped_chunks (move cold)
â”‚        â””â”€ CPU still has same memory usage!
â”‚           (just reorganized: hotâ†’cold)
â”‚
At this point:
â”œâ”€ GPU freed: gpu_used_bytes decreased âœ“
â”œâ”€ CPU not freed: cpu_used_bytes unchanged âœ“
â”‚  (chunks just moved from cache to dropped)
â””â”€ Recovery possible: tensors in dropped_chunks âœ“
```

---

## Code Changes Required

### Change 1: Don't subtract from cpu_used_bytes when moving to DROPPED

**File**: `src/pensieve/core/cache.py:536-542`

**Current** (WRONG):
```python
drop_chunk = self.cpu_cache.pop(drop_key)
self.cpu_used_bytes -= drop_chunk.size_bytes  # â† WRONG!
self.dropped_chunks[drop_key] = drop_chunk
```

**Fixed**:
```python
drop_chunk = self.cpu_cache.pop(drop_key)
# cpu_used_bytes NOT decreased
# (chunk moves from cache to dropped, both in CPU)
self.dropped_chunks[drop_key] = drop_chunk
```

Already partially fixed in previous edit. Need to remove the subtraction.

### Change 2: CPU eviction also shouldn't subtract

**File**: `src/pensieve/core/cache.py:559-564`

**Current** (WRONG):
```python
else:  # Evicting from CPU
    self.cpu_used_bytes -= chunk.size_bytes
    self.dropped_chunks[chunk_key] = chunk
```

**Should be**:
```python
else:  # Evicting from CPU
    # Chunk moves from cache to dropped
    # Both in CPU, so no memory freed
    self.dropped_chunks[chunk_key] = chunk
```

---

## Memory Hierarchy Semantics (CLARIFIED)

```
GPU Tier:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  gpu_cache (hot)     â”‚
â”‚  (in-use, fast)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   gpu_used_bytes

        â†“ (evict)

CPU Tier:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  cpu_cache (warm)    â”‚ â”€â”
â”‚  (reuse, medium)     â”‚  â”‚ cpu_used_bytes
â”‚                      â”‚  â”‚
â”‚  dropped_chunks      â”‚ â”€â”˜
â”‚  (cold recovery)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Eviction path:
GPU â†’ cpu_cache â†’ dropped_chunks
     (warm)       (cold, recovery)

Key: All CPU tiers count toward cpu_used_bytes!
```

---

## Session Lifetime and DROPPED Cleanup

```python
# Session ends:
evict_session(session_id):
    # 1. Remove from all active caches
    For each chunk_key in session_chunks[session_id]:
        if chunk_key in gpu_cache:
            gpu_cache.pop(chunk_key)
            gpu_used_bytes -= size  âœ“
        elif chunk_key in cpu_cache:
            cpu_cache.pop(chunk_key)
            cpu_used_bytes -= size  âœ“
        elif chunk_key in dropped_chunks:
            dropped_chunks.pop(chunk_key)
            # CPU memory freed! âœ“

    # 2. Remove tracking
    del session_chunks[session_id]

Result: When session ends, ALL chunks removed, CPU memory freed
```

---

## Memory Invariants (CORRECTED)

### Invariant 1: CPU Memory Accounting
```
cpu_used_bytes = Î£(cpu_cache) + Î£(dropped_chunks)
              = Memory actually used by CPU tier
```

### Invariant 2: No Memory Leaks
```
When session_id deleted:
  âˆ€ chunk_key âˆˆ session_chunks[session_id] (before delete):
    chunk completely removed from all tiers
    âˆ´ Memory freed
```

### Invariant 3: Recovery Available
```
âˆ€ chunk in dropped_chunks:
  chunk.tensors in CPU memory
  âˆ´ Recovery possible without I/O
```

---

## Design Trade-offs

### This Design (Option B)

âœ… **Pros**:
- Recovery is instant (no disk I/O)
- Simple implementation
- Matches Pensieve paper spirit
- Memory semantics clear after fix

âŒ **Cons**:
- DROPPED chunks consume CPU memory
- Eviction doesn't free memory (just reorganizes)
- Depends on session cleanup to free DROPPED

### Alternative (Option A - disk-backed)

âŒ **Why not**:
- Adds I/O latency to recovery
- Requires disk space management
- Serialization complexity
- Not justified for this prototype

---

## Recovery Mechanism

When session returns after eviction:

```
Recovery flow (from token_recovery.py):
1. detect_dropped_chunks(session_id)
   â”œâ”€ Find chunks in dropped_chunks
   â””â”€ Trigger recovery

2. recompute_dropped_chunks(session_id)
   â”œâ”€ Load tensors from dropped_chunks âœ“
   â”œâ”€ Run forward pass
   â””â”€ Update chunks back to CPU/GPU cache

3. Session continues with recovered KV

Benefit: Chunks recovered from memory, not recomputation
Overhead: Forward pass on dropped chunk tokens only
```

---

## Testing Memory Semantics

```python
def test_dropped_chunks_memory_semantics():
    """Verify DROPPED chunks are counted in cpu_used_bytes."""
    cache = TwoTierCache(gpu_capacity_gb=0.1, cpu_capacity_gb=0.2)

    # Fill CPU to trigger DROPPED
    for i in range(10):
        chunk = create_chunk("s1", i)
        cache.store_chunk(chunk, GPU)

    # Force some chunks to DROPPED
    initial_cpu_used = cache.cpu_used_bytes

    # Trigger eviction by adding more chunks
    for i in range(10, 20):
        chunk = create_chunk("s2", i-10)
        cache.store_chunk(chunk, GPU)

    # Check:
    # 1. Some s1 chunks should be in dropped_chunks
    s1_dropped = sum(1 for k in cache.dropped_chunks if "s1" in k)
    assert s1_dropped > 0, "Should have dropped s1 chunks"

    # 2. cpu_used_bytes should account for DROPPED
    total_cpu_chunks = (
        sum(c.size_bytes for c in cache.cpu_cache.values()) +
        sum(c.size_bytes for c in cache.dropped_chunks.values())
    )
    assert cache.cpu_used_bytes == total_cpu_chunks, \
        f"cpu_used_bytes mismatch: {cache.cpu_used_bytes} vs {total_cpu_chunks}"

    # 3. When session ends, both are freed
    cache.evict_session("s1")

    remaining_cpu = sum(c.size_bytes for c in cache.cpu_cache.values())
    assert cache.cpu_used_bytes == remaining_cpu, \
        "cpu_used_bytes should decrease when DROPPED chunks removed"
```

---

## Summary

**User's Question**: "ì›ë˜ DROPPED ê°™ì€ê±° ì•ˆ ì“°ê³ , ì•„ì˜ˆ removeí–ˆì§€ ì•Šì•„?"

**Answer**:
- âœ… DROPPED tierëŠ” ì˜ë„ëœ ì„¤ê³„ (F7 requirement)
- âŒ í•˜ì§€ë§Œ ë©”ëª¨ë¦¬ ì¶”ì ì´ ê¹¨ì ¸ìˆì—ˆìŒ
- ğŸ”§ **Fix**: DROPPED chunksë„ cpu_used_bytesì— í¬í•¨
- âœ… ê²°ê³¼: ë©”ëª¨ë¦¬ ì•ˆì „ì„± + Recovery capability í™•ë³´

Design is now **clear and consistent**!
